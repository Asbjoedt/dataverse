# Dataverse 6.5

Please note: To read these instructions in full, please go to https://github.com/IQSS/dataverse/releases/tag/v6.5 rather than the list of releases, which will cut them off.

This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.

## Release Highlights

New features in Dataverse 6.5:

- and more! Please see below.


## Features Added

### Longer Custom Questions in Guestbooks

Custom questions in Guestbooks can now be more than 255 characters and the bug causing a silent failure when questions were longer than this limit has been fixed. See also #9492, #10117, #10118.

### Private URL Renamed to Preview URL

With this release the name of the URL that may be used by dataset administrators to share a draft version of a dataset has been changed from Private URL to Preview URL.

Also, additional information about the creation of Preview URLs has been added to the popup accessed via edit menu of the Dataset Page.

Any Private URLs created in previous versions of Dataverse will continue to work.

The old "privateUrl" API endpoints for the creation and deletion of Preview (formerly Private) URLs have been deprecated. They will continue to work but please switch to the "previewUrl" equivalents that have been [documented](https://dataverse-guide--10961.org.readthedocs.build/en/10961/api/native-api.html#create-a-preview-url-for-a-dataset) in the API Guide.

See also #8184, #8185, #10950, and #10961.

## Multiple Language in Docker

Configuration and documentation has been added to explain how to set up multiple languages (e.g. English and French) in the tutorial for setting up Dataverse in Docker.

See also [the docs](https://dataverse-guide--10940.org.readthedocs.build/en/10940/container/running/demo.html#multiple-languages), #10939, and #10940.

### GlobusBatchLookupSize

An optimization has been added for the Globus upload workflow, with a corresponding new database setting: `:GlobusBatchLookupSize`

See the [Database Settings](https://guides.dataverse.org/en/6.5/installation/config.html#GlobusBatchLookupSize) section of the Guides, #10977, and #11040 for more information.

### Harvested Dataset PID from Record Header 

When harvesting, Dataverse can now use the identifier from the OAI-PMH record header as the persistent id for the harvested dataset.

This will allow harvesting from sources that do not include a persistent id in their oai_dc metadata records, but use valid DOIs or handles as the OAI-PMH record header identifiers. 

It is also possible to optionally configure a harvesting client to use this OAI-PMH identifier as the **preferred** choice for the persistent id. See the [Harvesting Clients API](https://guides.dataverse.org/en/6.5/api/native-api.html#create-a-harvesting-client) section of the Guides, #11049 and #10982 for more information.

## Bugs Fixed

### Updating Files Now Possible When Latest and Only Dataset Version is Deaccessioned

When a dataset was deaccessioned and was the only previous version it would cause an error when trying to update the files. This has been fixed. See #9351 and #10901.

### My Data Filter by Username Feature Fixed

The superuser-only feature of filtering by a username on the My Data page was not working. The "Results for Username" field now returns data for the desired user. See also #7239 and #10980.

### Version Differences Details Sorting Added

In order to facilitate the comparison between the draft version and the published version of a dataset, a sort on subfields has been added. See #10969.

### Facets Filter Labels Now Translated Above Search Results

On the main page, it's possible to filter results using search facets. If internationalization (i18n) has been activated in the Dataverse installation, allowing pages to be displayed in several languages, the facets are translated in the filter column. However, they weren't being translated above the search results, remaining in the default language, English.

This version of Dataverse fix this, and includes internationalization in the facets visible in the search results section. For more information, see #9408 and #10158.

### Unpublished File Bug Fix Related to Deaccessioning

A bug fix was made that gets the major version of a Dataset when all major versions were deaccessioned. This fixes the incorrect showing of the files as "Unpublished" in the search list even when they are published. This fix affects the indexing, meaning these datasets must be re-indexed once Dataverse is updated. See also #10947 and #10974.

### Minor DataCiteXML Fix

A minor bug fix was made to avoid sending a useless ", null" in the DataCiteXML sent to DataCite and in the DataCite export when a dataset has a metadata entry for "Software Name" and no entry for "Software Version". The bug fix will update datasets upon publication. Anyone with existing published datasets with this problem can be fixed by [pushing updated metadata to DataCite for affected datasets](https://guides.dataverse.org/en/6.5/admin/dataverses-datasets.html#update-metadata-for-a-published-dataset-at-the-pid-provider) and [re-exporting the dataset metadata](https://guides.dataverse.org/en/6.5/admin/metadataexport.html#batch-exports-through-the-api) or by steps 9 and 10 in the [v6.4 release notes](https://github.com/IQSS/dataverse/releases/tag/v6.4) to update and re-export all datasets. See also #10919.

### Globus "missing properties" Logging Fixed

In previous releases, logging would show Globus-related strings were missing from properties files. This has been fixed. See #11030 for details.

## API Updates

### Editing Collections

A new endpoint (`PUT /api/dataverses/<identifier>`) for updating an existing collection (dataverse) has been added. It uses the same JSON structure as the one used for collection creation. See also [the docs](https://preview.guides.gdcc.io/en/develop/api/native-api.html#update-a-dataverse-collection), #10904, and #10925.

### fileCount Added to Search API

A new search field called `fileCount` can be searched to discover the number of files per dataset. See also #8941 and #10598.

### List Dataset Metadata Exporters

A list of available dataset metadata exporters can now be retrieved programmatically via API. See [the docs](https://dataverse-guide--10739.org.readthedocs.build/en/10739/api/native-api.html#get-export-formats) and #10739.

### Audit Data Files

A superuser-only API endpoint has been added to audit datasets with data files where the physical files are missing or the file metadata is missing. See [the docs](https://dataverse-guide--11016.org.readthedocs.build/en/11016/api/native-api.html#datafile-audit), #11016, and [#220](https://github.com/IQSS/dataverse.harvard.edu/issues/220).

### Update Collection API Inheritance

The update collection (dataverse) API endpoint has been updated to support an "inherit from parent" configuration for metadata blocks, facets, and input levels.

Previously, not setting these fields meant using a copy of the settings from the parent collection, which could get out of sync.. See also [the docs](https://preview.guides.gdcc.io/en/develop/api/native-api.html#update-a-dataverse-collection), #11018, and #11026.

### isMetadataBlockRoot and isFacetRoot

The JSON payload of the "get collection" endpoint has been extended to include properties isMetadataBlockRoot and isFacetRoot. See also [the docs](https://preview.guides.gdcc.io/en/develop/api/native-api.html#view-a-dataverse-collection), #11012, and #11013.

### Get API Token Supports Any Auth

The `/api/users/token` endpoint has been extended to support any auth mechanism for retrieving the token information.

Previously, this endpoint only accepted an API token to retrieve its information. Now, it accepts any authentication mechanism and returns the associated API token information. See #10914 and #10924.

## Settings Added

- :GlobusBatchLookupSize

## Backward Incompatible Changes

Generally speaking, see the [API Changelog](https://preview.guides.gdcc.io/en/develop/api/changelog.html) for a list of backward-incompatible changes.

### List Collections Linked to a Dataset

The API endpoint that returns a list of collections that a dataset has been linked to has been improved to provide a more structured JSON response. See [the docs](https://dataverse-guide--9665.org.readthedocs.build/en/9665/admin/dataverses-datasets.html#list-collections-that-are-linked-from-a-dataset), #9650, and #9665.


## Complete List of Changes

For the complete list of code changes in this release, see the [6.5 milestone](https://github.com/IQSS/dataverse/issues?q=milestone%3A6.5+is%3Aclosed) in GitHub.

## Getting Help

For help with upgrading, installing, or general questions please post to the [Dataverse Community Google Group](https://groups.google.com/g/dataverse-community) or email support@dataverse.org.


## Installation

If this is a new installation, please follow our [Installation Guide](https://guides.dataverse.org/en/latest/installation/). Please don't be shy about [asking for help](https://guides.dataverse.org/en/latest/installation/intro.html#getting-help) if you need it!

Once you are in production, we would be delighted to update our [map of Dataverse installations](https://dataverse.org/installations) around the world to include yours! Please [create an issue](https://github.com/IQSS/dataverse-installations/issues) or email us at support@dataverse.org to join the club!

You are also very welcome to join the [Global Dataverse Community Consortium](https://www.gdcc.io/) (GDCC).


## Upgrade Instructions

Upgrading requires a maintenance window and downtime. Please plan accordingly, create backups of your database, etc.

These instructions assume that you've already upgraded through all the 5.x releases and are now running Dataverse 6.4.

0\. These instructions assume that you are upgrading from the immediate previous version. If you are running an earlier version, the only supported way to upgrade is to progress through the upgrades to all the releases in between before attempting the upgrade to this version.

If you are running Payara as a non-root user (and you should be!), **remember not to execute the commands below as root**. Use `sudo` to change to that user first. For example, `sudo -i -u dataverse` if `dataverse` is your dedicated application user.

In the following commands, we assume that Payara 6 is installed in `/usr/local/payara6`. If not, adjust as needed.

```shell
export PAYARA=/usr/local/payara6`
```

(or `setenv PAYARA /usr/local/payara6` if you are using a `csh`-like shell)

1\. Undeploy the previous version

```shell
$PAYARA/bin/asadmin undeploy dataverse-6.4
```

2\. Stop and start Payara

```shell
service payara stop
sudo service payara start
```

3\. Deploy this version

```shell
$PAYARA/bin/asadmin deploy dataverse-6.5.war
```

Note: if you have any trouble deploying, stop Payara, remove the following directories, start Payara, and try to deploy again.

```shell
service payara stop
rm -rf $PAYARA/glassfish/domains/domain1/generated
rm -rf $PAYARA/glassfish/domains/domain1/osgi-cache
rm -rf $PAYARA/glassfish/domains/domain1/lib/databases
```

4\. For installations with internationalization:

Please remember to update translations via [Dataverse language packs](https://github.com/GlobalDataverseCommunityConsortium/dataverse-language-packs).

5\. Restart Payara

```shell
service payara stop
service payara start
```

6\. Update metadata blocks

These changes reflect incremental improvements made to the handling of core metadata fields.

```shell
wget https://raw.githubusercontent.com/IQSS/dataverse/v6.4/scripts/api/data/metadatablocks/citation.tsv

curl http://localhost:8080/api/admin/datasetfield/load -H "Content-type: text/tab-separated-values" -X POST --upload-file citation.tsv
```

7\. Update Solr schema.xml file. Start with the standard v6.4 schema.xml, then, if your installation uses any custom or experimental metadata blocks, update it to include the extra fields (step 7a).

Stop Solr (usually `service solr stop`, depending on Solr installation/OS, see the [Installation Guide](https://guides.dataverse.org/en/6.4/installation/prerequisites.html#solr-init-script)).

```shell
service solr stop
```

Replace schema.xml

```shell
wget https://raw.githubusercontent.com/IQSS/dataverse/v6.4/conf/solr/schema.xml
cp schema.xml /usr/local/solr/solr-9.4.1/server/solr/collection1/conf
```

Start Solr (but if you use any custom metadata blocks, perform the next step, 7a first).

```shell
service solr start
```

7a\. For installations with custom or experimental metadata blocks:

Before starting Solr, update the schema to include all the extra metadata fields that your installation uses. We do this by collecting the output of the Dataverse schema API and feeding it to the `update-fields.sh` script that we supply, as in the example below (modify the command lines as needed to reflect the names of the directories, if different):

```shell
	wget https://raw.githubusercontent.com/IQSS/dataverse/v6.4/conf/solr/update-fields.sh
	chmod +x update-fields.sh
	curl "http://localhost:8080/api/admin/index/solr/schema" | ./update-fields.sh /usr/local/solr/solr-9.4.1/server/solr/collection1/conf/schema.xml
```

Now start Solr.

8\. Reindex Solr

Below is the simplest way to reindex Solr:

```shell
curl http://localhost:8080/api/admin/index
```

The API above rebuilds the existing index "in place". If you want to be absolutely sure that your index is up-to-date and consistent, you may consider wiping it clean and reindexing everything from scratch (see [the guides](https://guides.dataverse.org/en/latest/admin/solr-search-index.html)). Just note that, depending on the size of your database, a full reindex may take a while and the users will be seeing incomplete search results during that window.

9\. Run reExportAll to update dataset metadata exports

This step is necessary because of changes described above for the `Datacite` and `oai_dc` export formats.

Below is the simple way to reexport all dataset metadata. For more advanced usage, please see [the guides](http://guides.dataverse.org/en/6.4/admin/metadataexport.html#batch-exports-through-the-api).

```shell
curl http://localhost:8080/api/admin/metadata/reExportAll
```

10\. Pushing updated metadata to DataCite

(If you don't use DataCite, you can skip this.)

Above you updated the citation metadata block and Solr with the new "relationType" field. With these two changes, the "Relation Type" fields will be available and creation/publication of datasets will result in the expanded XML being sent to DataCite. You've also already run "reExportAll" to update the `Datacite` metadata export format.

Entries at DataCite for published datasets can be updated by a superuser using an API call (newly [documented](https://guides.dataverse.org/en/6.4/admin/dataverses-datasets.html#update-metadata-for-all-published-datasets-at-the-pid-provider)):

`curl  -X POST -H 'X-Dataverse-key:<key>' http://localhost:8080/api/datasets/modifyRegistrationPIDMetadataAll`

This will loop through all published datasets (and released files with PIDs). As long as the loop completes, the call will return a 200/OK response. Any PIDs for which the update fails can be found using the following command:

`grep 'Failure for id' server.log`

Failures may occur if PIDs were never registered, or if they were never made findable. Any such cases can be fixed manually in DataCite Fabrica or using the [Reserve a PID](https://guides.dataverse.org/en/6.4/api/native-api.html#reserve-a-pid) API call and the newly documented `/api/datasets/<id>/modifyRegistration` call respectively. See https://guides.dataverse.org/en/6.4/admin/dataverses-datasets.html#send-dataset-metadata-to-pid-provider. Please reach out with any questions.

PIDs can also be updated by a superuser on a per-dataset basis using

`curl -X POST -H 'X-Dataverse-key:<key>' http://localhost:8080/api/datasets/<id>/modifyRegistrationMetadata`

### Additional Upgrade Steps
